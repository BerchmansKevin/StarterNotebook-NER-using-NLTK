{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Berchmans`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Starter Notebook NER using NLTK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iYgy8wR3rrKO"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CLGGhS-BsF2T"
   },
   "outputs": [],
   "source": [
    "sentence1 = \"Kevin said on Monday that WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Renitta E. Lynch, the top federal prosecutor in Karthikeyan, spoke forcefully about the pain of a broken trust that African-Americans felt and Berchmans said the responsibility for repairing generations of Venkat miscommunication and mistrust fell to law enforcement.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lb-Q9d6QsdSt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\berch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\berch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\berch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\berch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXJor_h0eTXs"
   },
   "source": [
    "`The sentence needs to be tokenized and add the pos tags and the tags are parsed into chunk trees. The nltk library has a pre-trained namied entity chunker which can be done using ne_chunk() method. The below code is an example of how to chunk the sentence.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7z3XtkZXDDRG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Kevin/NNP)\n",
      "  said/VBD\n",
      "  on/IN\n",
      "  Monday/NNP\n",
      "  that/IN\n",
      "  (ORGANIZATION WASHINGTON/NNP)\n",
      "  --/:\n",
      "  In/IN\n",
      "  the/DT\n",
      "  wake/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  string/NN\n",
      "  of/IN\n",
      "  abuses/NNS\n",
      "  by/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  police/NN\n",
      "  officers/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  1990s/CD\n",
      "  ,/,\n",
      "  (PERSON Renitta/NNP E./NNP Lynch/NNP)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  top/JJ\n",
      "  federal/JJ\n",
      "  prosecutor/NN\n",
      "  in/IN\n",
      "  (GPE Karthikeyan/NNP)\n",
      "  ,/,\n",
      "  spoke/VBD\n",
      "  forcefully/RB\n",
      "  about/IN\n",
      "  the/DT\n",
      "  pain/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  broken/JJ\n",
      "  trust/NN\n",
      "  that/IN\n",
      "  African-Americans/NNP\n",
      "  felt/VBD\n",
      "  and/CC\n",
      "  (PERSON Berchmans/NNPS)\n",
      "  said/VBD\n",
      "  the/DT\n",
      "  responsibility/NN\n",
      "  for/IN\n",
      "  repairing/VBG\n",
      "  generations/NNS\n",
      "  of/IN\n",
      "  (GPE Venkat/NNP)\n",
      "  miscommunication/NN\n",
      "  and/CC\n",
      "  mistrust/NN\n",
      "  fell/VBD\n",
      "  to/TO\n",
      "  law/NN\n",
      "  enforcement/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence1)\n",
    "tags = pos_tag(tokens)\n",
    "ne_tree = ne_chunk(tags)\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXS7XfuRfK7t"
   },
   "source": [
    "`The code below is an example to count the NE but the desired output is shown in the next cell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kxUcW6q9skl4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'Kevin': 1, 'NNP': 1})]\n",
      "[Counter({'WASHINGTON': 1, 'NNP': 1})]\n",
      "[Counter({'New': 1, 'NNP': 1}), Counter({'York': 1, 'NNP': 1})]\n",
      "[Counter({'Renitta': 1, 'NNP': 1}), Counter({'E.': 1, 'NNP': 1}), Counter({'Lynch': 1, 'NNP': 1})]\n",
      "[Counter({'Karthikeyan': 1, 'NNP': 1})]\n",
      "[Counter({'Berchmans': 1, 'NNPS': 1})]\n",
      "[Counter({'Venkat': 1, 'NNP': 1})]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter   \n",
    "for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence1))):\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print([Counter(label) for label in chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BGzSzC0pEkSE"
   },
   "outputs": [],
   "source": [
    "#Desired Output\n",
    "#ORG:1\n",
    "#GPE:2\n",
    "#PERSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nq27RZZPgZX7"
   },
   "source": [
    "`The below two cells are examples to find entities through Regex patterns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MIA-AMLi4m8h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'Google', 'Google Search', 'Models', 'Berchmans', 'WASHINGTON', 'New York', 'police officers', 'Loretta E. Lynch', 'Brooklyn']\n"
     ]
    }
   ],
   "source": [
    "my_sent = \"Google to develop AI model supporting 1,000 popular global languages Google has already integrated 1000 popular languages across the world into Google Search, despite the criticism about their functionality. The Models of language were reportedly facing some flaws, like re-enacting harmful societal biases like xenophobia, racism and more,Berchmans said on Monday that WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "word = nltk.word_tokenize(my_sent)   \n",
    "pos_tag = nltk.pos_tag(word)   \n",
    "chunk = nltk.ne_chunk(pos_tag)  \n",
    "grammar = \"NP: {<NN><NNS>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]   \n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vIiriWhJ5HFq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'Google', 'the world', 'Google Search', 'the criticism', 'Models', 'Berchmans', 'WASHINGTON', 'the wake', 'a string', 'New York', 'Loretta E. Lynch', 'the top federal prosecutor', 'Brooklyn', 'the pain', 'a broken trust', 'the responsibility']\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT><JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]   \n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRJmpFdG_W3O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab 10 starter notebook: NER using nltk.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
